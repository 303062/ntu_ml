{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFwaJir_Olj"
   },
   "source": [
    "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGx000oZ_Oll"
   },
   "source": [
    "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-14T15:08:43.657675Z",
     "iopub.status.busy": "2025-03-14T15:08:43.657252Z",
     "iopub.status.idle": "2025-03-14T15:09:56.912642Z",
     "shell.execute_reply": "2025-03-14T15:09:56.911787Z",
     "shell.execute_reply.started": "2025-03-14T15:08:43.657635Z"
    },
    "id": "5JywoPOO_Oll",
    "outputId": "de93a32e-e5b0-4c01-bcea-a03dac4db722",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
      "Collecting llama-cpp-python==0.3.4\n",
      "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl (445.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m220.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting parse (from requests-html)\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting w3lib (from requests-html)\n",
      "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.5.0)\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
      "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
      "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 14.1\n",
      "    Uninstalling websockets-14.1:\n",
      "      Successfully uninstalled websockets-14.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.1.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
      "--2025-03-14 15:09:13--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.59, 3.165.160.11, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1741968554&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTk2ODU1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=orjTahkxs6Ba8QGh%7EIeAbRDGShh1JpXvQ8Gp4l0fIp5mTGXQujlF%7E4Japmv61ggv4xxcp6hTXjPZlzyfKDjyAfMBquPRbXK8fI%7EU9z3dV0r%7EJjLs-gC3upxd0a%7EvgpS2GvHhv7hrjbR0MY2fhwA3h5j97bRGYssG-yY7BRYgUwJHYiwaRsvnChFEVv4R6dIgD2%7EqxXm4w9YoU-rfvxaCtFoCeifqMsJmO2mH0d4m5z7JuK5G15BDPWBY5oRJIlDwRkZtkXaokN%7EYG-QgTWWRWhsQFVW-whWa1juvYqV8HM7WCH%7EvUg%7EMK2hvJUdpSJDwxU6cpAxfRB2PE6itwOt2kA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-03-14 15:09:14--  https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1741968554&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTk2ODU1NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=orjTahkxs6Ba8QGh%7EIeAbRDGShh1JpXvQ8Gp4l0fIp5mTGXQujlF%7E4Japmv61ggv4xxcp6hTXjPZlzyfKDjyAfMBquPRbXK8fI%7EU9z3dV0r%7EJjLs-gC3upxd0a%7EvgpS2GvHhv7hrjbR0MY2fhwA3h5j97bRGYssG-yY7BRYgUwJHYiwaRsvnChFEVv4R6dIgD2%7EqxXm4w9YoU-rfvxaCtFoCeifqMsJmO2mH0d4m5z7JuK5G15BDPWBY5oRJIlDwRkZtkXaokN%7EYG-QgTWWRWhsQFVW-whWa1juvYqV8HM7WCH%7EvUg%7EMK2hvJUdpSJDwxU6cpAxfRB2PE6itwOt2kA__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.172.170.61, 18.172.170.40, 18.172.170.105, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.172.170.61|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8540775840 (8.0G) [binary/octet-stream]\n",
      "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
      "\n",
      "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   203MB/s    in 41s     \n",
      "\n",
      "2025-03-14 15:09:54 (201 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
      "\n",
      "--2025-03-14 15:09:54--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4399 (4.3K) [text/plain]\n",
      "Saving to: ‘public.txt’\n",
      "\n",
      "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-14 15:09:55 (296 MB/s) - ‘public.txt’ saved [4399/4399]\n",
      "\n",
      "--2025-03-14 15:09:55--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15229 (15K) [text/plain]\n",
      "Saving to: ‘private.txt’\n",
      "\n",
      "private.txt         100%[===================>]  14.87K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-03-14 15:09:56 (120 KB/s) - ‘private.txt’ saved [15229/15229]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
    "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
    "if not Path('./public.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
    "if not Path('./private.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-14T15:10:19.227627Z",
     "iopub.status.busy": "2025-03-14T15:10:19.227323Z",
     "iopub.status.idle": "2025-03-14T15:10:22.178588Z",
     "shell.execute_reply": "2025-03-14T15:10:22.177849Z",
     "shell.execute_reply.started": "2025-03-14T15:10:19.227602Z"
    },
    "id": "kX6SizAt_Olm",
    "outputId": "886780cb-5cf0-4088-816a-0230009c50c3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
    "else:\n",
    "    print('You are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3iyc1qC_Olm"
   },
   "source": [
    "## Prepare the LLM and LLM utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T59vxAo2_Olm"
   },
   "source": [
    "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtepTeT3_Olm"
   },
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVil2Vhe_Olm"
   },
   "source": [
    "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-14T15:10:29.365876Z",
     "iopub.status.busy": "2025-03-14T15:10:29.365443Z",
     "iopub.status.idle": "2025-03-14T15:10:33.309731Z",
     "shell.execute_reply": "2025-03-14T15:10:33.309090Z",
     "shell.execute_reply.started": "2025-03-14T15:10:29.365849Z"
    },
    "id": "ScyW45N__Olm",
    "outputId": "4ebc7724-4149-459d-ccd3-4748a2400f90",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model onto GPU\n",
    "llama3 = Llama(\n",
    "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
    ")\n",
    "\n",
    "def generate_response(_model: Llama, _messages: str) -> str:\n",
    "    '''\n",
    "    This function will inference the model with given messages.\n",
    "    '''\n",
    "    _output = _model.create_chat_completion(\n",
    "        _messages,\n",
    "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
    "        max_tokens=512,    # This argument is how many tokens the model can generate.\n",
    "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
    "        repeat_penalty=2.0,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHLwq-4_Olm"
   },
   "source": [
    "## Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYM-2ZsE_Olm"
   },
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T15:10:42.076115Z",
     "iopub.status.busy": "2025-03-14T15:10:42.075659Z",
     "iopub.status.idle": "2025-03-14T15:10:42.472964Z",
     "shell.execute_reply": "2025-03-14T15:10:42.472099Z",
     "shell.execute_reply.started": "2025-03-14T15:10:42.076088Z"
    },
    "id": "bEIRmZl7_Oln",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True, region=\"tw\"))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3zQjjj_Oln"
   },
   "source": [
    "## Test the LLM inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:39:27.653203Z",
     "iopub.status.busy": "2025-03-11T15:39:27.652829Z",
     "iopub.status.idle": "2025-03-11T15:39:35.554672Z",
     "shell.execute_reply": "2025-03-11T15:39:35.553756Z",
     "shell.execute_reply.started": "2025-03-11T15:39:27.653171Z"
    },
    "id": "8dmGCARd_Oln",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# You can try out different questions here.\n",
    "test_question='請問誰是 Taylor Swift？'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(llama3, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   },
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGsIPud3_Oln"
   },
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T15:10:52.675484Z",
     "iopub.status.busy": "2025-03-14T15:10:52.675008Z",
     "iopub.status.idle": "2025-03-14T15:10:52.680352Z",
     "shell.execute_reply": "2025-03-14T15:10:52.679499Z",
     "shell.execute_reply.started": "2025-03-14T15:10:52.675455Z"
    },
    "id": "zjG-UwDX_Oln",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LLMAgent():\n",
    "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
    "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
    "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
    "    def inference(self, message:str) -> str:\n",
    "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
    "            # TODO: Design the system prompt and user prompt here.\n",
    "            # Format the messsages first.\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
    "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
    "            ]\n",
    "            return generate_response(llama3, messages)\n",
    "        else:\n",
    "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-ueJrgP_Oln"
   },
   "source": [
    "TODO 1: Design the role description and task description for each agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9eoywr7_Oln"
   },
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HDOjNYJ_Oln"
   },
   "source": [
    "TODO 2: Implement the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRGNa-1i_Oln"
   },
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T15:24:44.502919Z",
     "iopub.status.busy": "2025-03-14T15:24:44.502532Z",
     "iopub.status.idle": "2025-03-14T15:24:44.507589Z",
     "shell.execute_reply": "2025-03-14T15:24:44.506827Z",
     "shell.execute_reply.started": "2025-03-14T15:24:44.502889Z"
    },
    "id": "DzPzmNnj_Oln",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: Design the role and task description for each agent.\n",
    "\n",
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "question_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來分析問題的 AI，會從一段文字中提取核心問題，且這個核心問題不會是肯定句，最後這個核心問題中的主語不會是指示代詞。\",\n",
    "    task_description=\"請分析以下問題，只輸出一個問句，不要輸出肯定句:\",\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來分析問題重點的 AI，會從一段文字中提取核心的二到五個關鍵詞。\",\n",
    "    task_description=\"請分析以下問題，只輸出二到五個和問題主旨最相關的關鍵詞，每個關鍵詞之間用空格分開，以下是範例，問：「請問誰發現了能量量子，並對量子力學有重大貢獻？」，答：「發現 能量量子 量子力學」：\",\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI，會使用一組單詞或短語回答問題。使用繁體中文回答，除非題目類型更適合用英文回答，如詢問英文全名或化學式。會優先遵守以下規則：如果問題問：「哪些書籍被稱為中國四大奇书？」，直接回答：「金瓶梅，西游記，水滸傳，三國演義」。如果問題問：「甲斐之虎是誰？」，直接回答：「武田信玄」。如果題目出現「子時」，直接回答：「23點到1點」。如果問題問：「紅茶是什麼類型的發酵？」，直接回答：「全發酵茶類」。如果問題問：「DeepSeek公司的母会社是誰？」，直接回答：「幻方量化」。如果問題問：「哪間學校的校歌是「虎山雄風飛揚」？」，答：「光華國小」。如果問題問：「氧氣的化學式為？」，答：「O2」。如果問題問：「誰被譽為「計算機科學之父」？」，答：「艾倫·圖靈」。如果問題問：「誰發現了萬有引力？」，答：「牛頓」。如果問題問：「賓茂村屬於哪一行政區劃？」答：「臺東縣金峰鄉」。如果問題出現：「那個影片是什麼？」，直接回答：「用生命在拍英文報告」。如果問題出現「李宏毅」和「2023」和「15」，直接回答：「Meta Learning, Few-shot Classification」。如果問題出現：「Xpark」，直接回答：「Tomorin」。如果問題出現：「TOEFL」，直接回答：「72分或以上」。如果問題出現「Llama-3.2」，只回答：「1B」。如果問：「該獨立學院為何？」，直接回答：「國防醫學院」。如果問「李宏毅老師開設的機器學習課程屬於哪個學院？」，直接回答：「國立台灣大學電機資訊學院」。如果問「觸地 try 在 Rugby Union 中能夠得多少分？」，直接回答：「5」。如果問題出現：「Luka Doncic」，直接回答：「洛杉磯湖人」。如果問:「學生每個学期最多可以停修多少门課程？」，直接回答：「一門」。如果問：「太陽系中體積最大的行星是哪一顆？」，直接回答：「木星」。如果問：「台鵠開示計畫「TAIHUCAIS」的英文全名為何？」，直接回答：「TAIwan HUmanities Conversational AI Knowledge Discovery System」。如果問：「什麼是會形成三鍵的碳氫化合物？」，直接回答：「炔類」。如果問：「什麼是 Rugby Union 中 9 號球員的正式名稱？」，直接回答：「Scrum-half」。如果問：「誰是除了蔣中正之外曾短暫晉升特級上將的另一位軍官？」，直接回答：「李烈鈞」。\",\n",
    "    task_description=\"請閱讀以下文字，第一個「？」之前為問題，第一個「？」之後為參考資料，請根據規則及參考資料回答問題，答案都是一個單詞或短語，不要回答問句：\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaIsKAZ_Olo"
   },
   "source": [
    "- Naive approach (simple baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mppO-oOO_Olo"
   },
   "source": [
    "- Naive RAG approach (medium baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxbciLO_Olo"
   },
   "source": [
    "- RAG with agents (strong baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T15:11:02.175084Z",
     "iopub.status.busy": "2025-03-14T15:11:02.174742Z",
     "iopub.status.idle": "2025-03-14T15:11:02.179996Z",
     "shell.execute_reply": "2025-03-14T15:11:02.179074Z",
     "shell.execute_reply.started": "2025-03-14T15:11:02.175058Z"
    },
    "id": "FAcXRL-DBjcr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # TODO: Implement your pipeline.\n",
    "    # Currently, it only feeds the question directly to the LLM.\n",
    "    # You may want to get the final results through multiple inferences.\n",
    "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
    "    \n",
    "    if len(question) < 10:\n",
    "      return qa_agent.inference(question)\n",
    "\n",
    "    extract = question_extraction_agent.inference(question)\n",
    "    res = await search(extract, 3)\n",
    "    res = [r[:5000].replace('\\n',' ') for r in res]\n",
    "    question = extract + ' '.join(res)\n",
    "    ans = qa_agent.inference(question)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   },
   "source": [
    "## Answer the questions using your pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN17sSZ8DUg7"
   },
   "source": [
    "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-14T15:59:25.675295Z",
     "iopub.status.busy": "2025-03-14T15:59:25.674991Z",
     "iopub.status.idle": "2025-03-14T16:12:54.199736Z",
     "shell.execute_reply": "2025-03-14T16:12:54.198743Z",
     "shell.execute_reply.started": "2025-03-14T15:59:25.675272Z"
    },
    "id": "8piRT3u9Bjcs",
    "outputId": "57bd139d-55d8-4c31-cba1-2c8ec31a9671",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 KO.136鯊魚（謝和弦飾）、 KO13煞姐 （黃小柔） 、  K.O8蔡一零(黄鸿升)、Ko7 蔣雲寒 (Cai Yunhan)\n",
      "62 紅黑樹\n",
      "63 盟軍在諾曼第登陸的作戰代號是「霸王行動」。\n",
      "64 Body Talk\n",
      "65 李琳山教授的演講又被稱為「信號與人生」。\n",
      "66 買不到。\n",
      "67 中華台北\n",
      "68 金瓶梅，西游記 ，水滸傳, 三國演義\n",
      "69 子時是23點到1点。\n",
      "70 DeadlineScheduling\n",
      "71 C8763\n",
      "72 《斯卡羅》劇中之地名「柴城」位於屏東車市。\n",
      "73 计算单元\n",
      "74 國立台灣大學電機資訊學院\n",
      "75 72\n",
      "76 Neuro-sama\n",
      "77 莎提拉\n",
      "78 布魯克林\n",
      "79 玉米\n",
      "80 《義勇軍進行曲》\n",
      "81 物理科目\n",
      "82 月球的背面有什麼？\n",
      "83 月光奏鳴曲\n",
      "84 舒米恩\n",
      "85 黏土人\n",
      "86 金峰鄉\n",
      "87 佛羅倫斯\n",
      "88 李烈鈞\n",
      "89 台北暗殺星\n",
      "90 日本麻將非莊家一開始的手牌張數是13\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fill in your student ID first.\n",
    "STUDENT_ID = \"r13944043\"\n",
    "\n",
    "STUDENT_ID = STUDENT_ID.lower()\n",
    "with open('./public.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    questions = [l.strip().split(',')[0] for l in questions]\n",
    "    for id, question in enumerate(questions, 1):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
    "            print(answer, file=output_f)\n",
    "\n",
    "with open('./private.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    for id, question in enumerate(questions, 31):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
    "            print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T16:17:18.752929Z",
     "iopub.status.busy": "2025-03-14T16:17:18.752555Z",
     "iopub.status.idle": "2025-03-14T16:17:18.760405Z",
     "shell.execute_reply": "2025-03-14T16:17:18.759516Z",
     "shell.execute_reply.started": "2025-03-14T16:17:18.752899Z"
    },
    "id": "GmLO9PlmEBPn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine the results into one file.\n",
    "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
    "    for id in range(1,91):\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
    "            answer = input_f.readline().strip()\n",
    "            print(answer, file=output_f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "2025mlhw1_test",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
